# 2020-07-01 DS FAQ
-
  Q: "
  As per our discussion last night I've cleaned the target variable, developed an initial model (I used a logistic regression rather than a linear regression) and loaded the text in to the dataframe. Do you have any recommendations for NLP models or EDA I can examine before our next meeting? See the notebook here: https://github.com/HechtYehuda/Hate-speech-modeler/blob/master/Hate%20speech%20modeler.ipynb  #email #student #YeHe"
  A: "
  Excellent! Good choice on logistic regression.
  1. First, I would create an new numerical feature based on the text column. Something like the length of the text (number of characters) or the number words.
  2. Retrain your model and with both your features and see if accuracy improves.
  3. Then look for the presence or absence of a hateful word in each string and make that a new binary feature. retrain and get an accuracy.
  4. Then look at using CountVectorizer to do that automatically on all the words in your texts.
  5. Then use TfidfVectorizer to divide that count by the number of documents each word is in.
  6. At this point you'll get great accuracy. But it will be overfitting. So you'll need to do your train test split to see how badly it does on a test set.
  7. Once you have a training set and test set and a TFIDF vectorizer tuned up (playing with min_df and max_df to maximize testset accuracy) then you'll need to read up on more advanced techniques to reduce dimensionality for NL data.    #email #student #YeHe"
-
  Q: "The [shap package](https://github.com/slundberg/shap) produces nice feature maps for image classification deep learning. The `DeepExplainer.image_plot()` overlays red pixels for regions where the filter (convolutional layer output) was negatively correlated with the target class. It uses blue to indicate positive correlation with the target class. But my rectangular images seem to be misaligned with the red and blue overlay of feature importances from shap."
  A: "The shap package may have a bug or you may need to rotate your images before passing them to the image_plot(). `DeepExplainer.image_plot() works fine on the MNIST dataset which is square. So try clipping your images to make them square before passing into shap."
-
  Q: My git commits aren't appearing on my repository on gitlab.  #student
  A: What error messages do you see?  #teacher #socratic
  Q1: "When I type `git push` it just says \"Everything up-to-date\"" #student
  A1: What does `git status` say?
  Q2: "When I type `git status` it shows the file I changed in red and says the changes are not yet staged to be committed" #student
  A2: How do you do what it says and stage those changes to be committed? #socratic #teacher
  Q3: Using the `git add` command or using the `--all` or `-m`
-
  Q: Should I use RandomForestRegressor or RandomForestClassifier for my 911 response time machine learning model? #student
  A: What is the difference between a _classifier_ and a _regressor_? #teacher #socratic
  Q1: My problem has a variety of categorical and numerical features. A regressor uses those features to predict the probability of a variable falling into a particular bucket or class or category. But my model is predicting a numerical quantity, the response time in minutes. It's not a probability, so should I use a classifier? #student
  A1: You're probably thinking of a famous classifier called LogisticRegression. The _LogisticRegression_ name is unfortunate, because it does predict a continuous value, the probability of being in a particular class. But it's purpose and use is as a classifier. It predicts this class (or category, or bucket) in which a particular example belongs. So your problem that is predicting a numerical quantity like time in minutes is not a classifier. #teacher #socratic
  Q2: So I should use a RandomForestRegressor to predict the response time for 911 calls? #student
  A2: Yes, whenever your target variable is a continuous numerical value, a regressor is the right kind of model to use. #teacher
-
  Q: When do I reject the null hypothesis and accept the alternative hypothesis that I proposed at the beginning of the project?
  A: When the P-value for your results is below 0.05 (5%) you can usually reject the null hypothesis. This means that your results are statistically significant and not likely to be caused by chance and your hypothesis represents a probable relationship between your feature variable and your target variable proposed in your hypothesis.
-
  Q: What does a good data story look like?  #student
  A: What kind of story does your audience like to hear? Try making it interesting by setting the stage with a mystery, a plot of your data that looks interesting or unusual or boring. Then say something that interests you about it. Talk about surprising relationships between variables or unusual shapes or values or outliers in the plots. Look for patterns in the plots. First describe those patterns. Then make some guesses at what causes those patterns and how they might be useful in understanding the real world, which is what your stakeholders and the audience for your data story cares about.  #teacher #socratic
-
  Q: How do I come up with a problem statement for data science?  #student
  A: "Start with the data. What columns do you have? What kinds of variables are there in each column? Categorical? Numerical? If you have a categorical variable and a numerical variable you can start with the question (problem statement) \"What is the mean of the numerical variable within each of the categories for the categorical variable?\" #teacher"
-
  Q: If your model to predict the prices of something has a standard error of $42, how often are your predictions to underestimate the price by more than $42?
  A: 16% of the time your predictions will be lower than 1 sigma ($42) below the true price. This can be calculated using your understanding of the 68, 95, 99.7 rule. 100% minus 68%/2 plus 50% is 100 - 84 which is 16%.
-
  Q: What is a good exercise to get up to speed on Data Science?
  A: Load a DataFrame of home prices and calculate some mean and average prices in various categories of homes.
-
  Q: When you are interviewing a Data Scientist what do you look for?
  A: I like to have the candidate talk about their thinking as they solve a data science problem for me verbally. I will describe a dataset and a problem I'm having at work and ask how they would solve it. I look for good data science habbits, like asking me questions about the data, and an agile software development and data science approach. I look for them to pay attention to the both test set error and the training set error to make decisions about what to do next after each new feature engineering or preprocessing step.
-
  Q: When should I use lemmatization on a sentiment analysis or other NLP problem?  #student
  A: When it improves your test set performance metric like F1 score or accuracy.  #teacher
  Q1: Does lemmatization reduce or increase the number of features in your dataset?  #teacher #socratic
  A1: It reduces the number of features, so it can help with overfitting.
-
  Q: About regularization. Why does reducing the values of coefficients from the optimal solution cause the accuracy to go up?
  A: Regularization doesn't improve the accuracy for all of your data. It will always make your training set accuracy worse. And sometimes it can even make your test set accuracy worse. But in some situations it will help your model avoid paying too much attention to your training set and trying to fit it too closely (called over fitting). This will sometime help it pay attention to the features that are most important in the test set. So sometimes it may help close the gap between your training set accuracy and your test set accuracy by increasing your test set accuracy and reducing your training set accuracy.
-
  Q: I'm starting to learn Data Science and python software development. What operating system should I install on my laptop?
  A: If you chose a Linux operating system, you'll never have to learn how to use another one. Linux based OSes (Operating Systems) like Ubuntu, Mint, Centos, and Alpine will always be here. And most of the servers and supercomputers you use for Data Science will be Linux-based. Plus you'll keep manipulative, deceptive, extractive corporations and their closed APIs and expensive products out of your life. Embrace the penguin! You won't regret it.
